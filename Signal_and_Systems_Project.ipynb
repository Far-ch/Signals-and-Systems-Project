{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Far-ch/Signals-and-Systems-Project/blob/main/Signal_and_Systems_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#YOLO\n",
        "from ultralytics import YOLO\n",
        "import cv2, torch, numpy as np, matplotlib.pyplot as plt\n",
        "\n",
        "VIDEO_IN  = \"person1.mp4\"\n",
        "SNAPSHOT  = \"first_detected.jpg\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL  = \"yolov8m.pt\"\n",
        "model  = YOLO(MODEL).to(DEVICE)\n",
        "CLASSES = {\n",
        "    0: \"person\",1: \"bicycle\",2: \"car\",3: \"motorcycle\",4: \"airplane\",5: \"bus\",6: \"train\",7: \"truck\",8: \"boat\",9: \"traffic light\",10: \"fire hydrant\",\n",
        "11: \"stop sign\", 12: \"parking meter\",13: \"bench\",14: \"bird\",15: \"cat\",16: \"dog\",17: \"horse\",18: \"sheep\",19: \"cow\",20: \"elephant\",21: \"bear\",\n",
        "22: \"zebra\",23: \"giraffe\",24: \"backpack\",25: \"umbrella\",26: \"handbag\",27: \"tie\",28: \"suitcase\",29: \"frisbee\",30: \"skis\",31: \"snowboard\",32: \"sports ball\",\n",
        "33: \"kite\",34: \"baseball bat\",35: \"baseball glove\",36: \"skateboard\",37: \"surfboard\",38: \"tennis racket\",39: \"bottle\",40: \"wine glass\",41: \"cup\",\n",
        "42: \"fork\",43: \"knife\",44: \"spoon\",45: \"bowl\",46: \"banana\",47: \"apple\",48: \"sandwich\",49: \"orange\",50: \"broccoli\",51: \"carrot\",\n",
        "52: \"hot dog\", 53: \"pizza\",54: \"donut\",55: \"cake\",56: \"chair\",57: \"couch\",58: \"potted plant\",59: \"bed\",60: \"dining table\",61: \"toilet\",\n",
        "62: \"tv\",63: \"laptop\",64: \"mouse\",65: \"remote\",66: \"keyboard\",67: \"cell phone\",68: \"microwave\",69: \"oven\",70: \"toaster\",71: \"sink\",72: \"refrigerator\",\n",
        "73: \"book\",74: \"clock\",75: \"vase\",76: \"scissors\",77: \"teddy bear\",78: \"hair drier\",79: \"toothbrush\"}\n",
        "\n",
        "CLASS_IDS = list(CLASSES.keys())\n",
        "rng = np.random.default_rng(42)\n",
        "COLORS = {cls_id: tuple(rng.integers(64, 256, 3).tolist()) for cls_id in CLASS_IDS}\n",
        "\n",
        "BOX_THICK = 3\n",
        "FONT_SCALE = 1.0\n",
        "FONT_THICK = 2\n",
        "\n",
        "\n",
        "def detect(img_rgb, imgsz=960, conf=0.3):\n",
        "    \"\"\"Detect objects and return list of boxes: (x1, y1, x2, y2, class_id, conf)\"\"\"\n",
        "    res = model.predict(img_rgb, imgsz=imgsz, conf=conf,\n",
        "                        classes=CLASS_IDS, device=DEVICE,\n",
        "                        verbose=False)[0]\n",
        "    out = []\n",
        "    for b in res.boxes:\n",
        "        out.append((*map(int, b.xyxy[0]), int(b.cls[0]), float(b.conf[0])))\n",
        "    return out\n",
        "\n",
        "\n",
        "def robust_detect(frame_bgr):\n",
        "    H, W = frame_bgr.shape[:2]\n",
        "\n",
        "    #Full-frame\n",
        "    boxes = detect(cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB), 1280, 0.3)\n",
        "    if boxes:\n",
        "        return boxes\n",
        "\n",
        "    #Center crop + upscale\n",
        "    cw, ch = int(W * 0.6), int(H * 0.7)\n",
        "    x0, y0 = (W - cw) // 2, (H - ch) // 2\n",
        "    crop = frame_bgr[y0:y0+ch, x0:x0+cw]\n",
        "    crop_up = cv2.resize(crop, (1280, 1280))\n",
        "    boxes_crop = detect(cv2.cvtColor(crop_up, cv2.COLOR_BGR2RGB), 1280, 0.25)\n",
        "    if boxes_crop:\n",
        "        sx, sy = cw / 1280, ch / 1280\n",
        "        return [(int(x1*sx + x0), int(y1*sy + y0),\n",
        "                 int(x2*sx + x0), int(y2*sy + y0), cls, conf)\n",
        "                for x1, y1, x2, y2, cls, conf in boxes_crop]\n",
        "\n",
        "    #Sliding tiles\n",
        "    out = []\n",
        "    tile, stride = 640, 320\n",
        "    for y in range(0, H, stride):\n",
        "        for x in range(0, W, stride):\n",
        "            tile_bgr = frame_bgr[y:y+tile, x:x+tile]\n",
        "            boxes_tile = detect(cv2.cvtColor(tile_bgr, cv2.COLOR_BGR2RGB), 640, 0.25)\n",
        "            for x1, y1, x2, y2, cls, conf in boxes_tile:\n",
        "                out.append((x1+x, y1+y, x2+x, y2+y, cls, conf))\n",
        "    return out\n",
        "\n",
        "\n",
        "cap = cv2.VideoCapture(VIDEO_IN)\n",
        "ok, frame = cap.read()\n",
        "cap.release()\n",
        "assert ok, f\" Couldn't read first frame of {VIDEO_IN}\"\n",
        "\n",
        "boxes = robust_detect(frame.copy())\n",
        "for x1, y1, x2, y2, cls, conf in boxes:\n",
        "    label_name = CLASSES.get(cls, \"unknown\")\n",
        "    label_text = f\"{label_name} {conf:.2f}\"\n",
        "\n",
        "    color = COLORS.get(cls, (0, 255, 255))\n",
        "    cv2.rectangle(frame, (x1, y1), (x2, y2), color, BOX_THICK)\n",
        "\n",
        "    (tw, th), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                                  FONT_SCALE, FONT_THICK)\n",
        "    cv2.rectangle(frame, (x1, y1 - th - 8), (x1 + tw, y1), color, -1)\n",
        "    cv2.putText(frame, label_text, (x1, y1 - 5),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, FONT_SCALE,\n",
        "                (255, 255, 255), FONT_THICK, cv2.LINE_AA)\n",
        "\n",
        "\n",
        "cv2.imwrite(SNAPSHOT, frame)\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "73hPLvv5OYkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fast-R-CNN\n",
        "!pip -q install --upgrade torch torchvision torchaudio\n",
        "import cv2, torch, matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import models, transforms\n",
        "\n",
        "# COCO classes\n",
        "CLASSES = {\n",
        "    0:\"background\", 1:\"person\", 2:\"bicycle\", 3:\"car\", 4:\"motorcycle\", 5:\"airplane\",\n",
        "    6:\"bus\",7:\"train\",8:\"truck\",9:\"boat\",10:\"trafficlight\",11:\"firehydrant\",\n",
        "    13:\"bench\",14:\"bird\",15:\"cat\",16:\"dog\",17:\"horse\",18:\"sheep\",19:\"cow\",\n",
        "    20:\"elephant\",21:\"bear\",22:\"zebra\",23:\"giraffe\",24:\"backpack\",25:\"umbrella\",\n",
        "    27:\"handbag\",28:\"tie\",31:\"snowboard\",32:\"sportsball\",33:\"kite\",34:\"baseballbat\",\n",
        "    35:\"baseballglove\",36:\"skateboard\",37:\"surfboard\",38:\"tennisracket\",39:\"bottle\",\n",
        "    40:\"wineglass\",41:\"cup\",42:\"fork\",43:\"knife\",44:\"spoon\",45:\"bowl\",46:\"banana\",\n",
        "    47:\"apple\",48:\"sandwich\",49:\"orange\",50:\"broccoli\",51:\"carrot\",52:\"hotdog\",\n",
        "    53:\"pizza\",54:\"donut\",55:\"cake\",56:\"chair\",57:\"couch\",58:\"pottedplant\",59:\"bed\",\n",
        "    60:\"diningtable\",61:\"toilet\",62:\"tv\",63:\"laptop\",64:\"mouse\",65:\"remote\",\n",
        "    66:\"keyboard\",67:\"cellphone\",68:\"microwave\",69:\"oven\",70:\"toaster\",71:\"sink\",\n",
        "    72:\"refrigerator\",73:\"book\",74:\"clock\",75:\"vase\",76:\"scissors\",77:\"teddybear\",\n",
        "    78:\"hairdrier\",79:\"toothbrush\"\n",
        "}\n",
        "\n",
        "VIDEO_PATH = \"person1.mp4\"\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "ok, frame_bgr = cap.read()\n",
        "cap.release()\n",
        "assert ok, \"Could not read first frame\"\n",
        "\n",
        "frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model  = models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "model.to(device).eval()\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])   # converts to [0,1] + CHW\n",
        "img_tensor = transform(frame_rgb).to(device)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = model([img_tensor])[0]   # dict with boxes, labels, scores\n",
        "\n",
        "boxes   = preds[\"boxes\"].cpu().numpy()\n",
        "labels  = preds[\"labels\"].cpu().numpy()\n",
        "scores  = preds[\"scores\"].cpu().numpy()\n",
        "\n",
        "CONF_TH = 0.50\n",
        "keep = scores >= CONF_TH\n",
        "\n",
        "boxes, labels, scores = boxes[keep], labels[keep], scores[keep]\n",
        "\n",
        "OUT = frame_bgr.copy()\n",
        "for (x1,y1,x2,y2), cls_id, conf in zip(boxes, labels, scores):\n",
        "    cls_id = int(cls_id)\n",
        "    name   = CLASSES.get(cls_id, f\"id{cls_id}\")\n",
        "    label  = f\"{name} {conf:.2f}\"\n",
        "\n",
        "    color = (0,255,0)   # green boxes; change if you like\n",
        "    cv2.rectangle(OUT, (int(x1),int(y1)), (int(x2),int(y2)), color, 2)\n",
        "    (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
        "    cv2.rectangle(OUT, (int(x1), int(y1)-th-8), (int(x1)+tw, int(y1)), color, -1)\n",
        "    cv2.putText(OUT, label, (int(x1), int(y1)-4),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 2)\n",
        "\n",
        "cv2.imwrite(\"fast_rcnn_detect.jpg\", OUT[:, :, ::-1])  # BGRâ†’RGB file\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.imshow(cv2.cvtColor(OUT, cv2.COLOR_BGR2RGB))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Fast-R-CNN detection \")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LmYF8EyTPOEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MOSSE_Tracker\n",
        "import time\n",
        "import uuid\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from pathlib import Path\n",
        "\n",
        "#PATHS\n",
        "downloads = Path.home() / \"Downloads\"\n",
        "VIDEO_PATH = downloads / \"videos\" / \"car1.mp4\"\n",
        "OUTPUT_PATH = downloads / \"car1_mosse_final_output.mp4\"\n",
        "\n",
        "#PARAMETERS\n",
        "CONF_TH = 0.5\n",
        "MAX_LOST = 250\n",
        "HIST_THRESH = 0.6\n",
        "HIST_BINS = (16, 16, 16)\n",
        "\n",
        "#COCO LABELS\n",
        "COCO = {i: name for i, name in enumerate([\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', '', 'stop sign',\n",
        "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', '', 'backpack', 'umbrella', '', '',\n",
        "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
        "    'tennis racket', 'bottle', '', 'wine glass', 'cup', 'fork', 'knife',\n",
        "    'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli',\n",
        "    'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "    'potted plant', 'bed', '', 'dining table', '', '', 'toilet', '', 'tv',\n",
        "    'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
        "    'oven', 'toaster', 'sink', 'refrigerator', '', 'book', 'clock', 'vase',\n",
        "    'scissors', 'teddy bear', 'hair drier', 'toothbrush'])}\n",
        "\n",
        "#Histogram functions\n",
        "def get_histogram(patch):\n",
        "    hsv = cv2.cvtColor(patch, cv2.COLOR_BGR2HSV)\n",
        "    hist = cv2.calcHist([hsv], [0, 1, 2], None, HIST_BINS, [0, 180, 0, 256, 0, 256])\n",
        "    cv2.normalize(hist, hist)\n",
        "    return hist.flatten()\n",
        "\n",
        "def compare_hist(hist1, hist2):\n",
        "    return cv2.compareHist(hist1.astype(np.float32), hist2.astype(np.float32), cv2.HISTCMP_CORREL)\n",
        "\n",
        "#Kalman Filter\n",
        "class KalmanFilter:\n",
        "    def __init__(self, x, y):\n",
        "        self.kf = cv2.KalmanFilter(8, 4)\n",
        "        self.kf.transitionMatrix = np.eye(8, dtype=np.float32)\n",
        "        for i in range(4):\n",
        "            self.kf.transitionMatrix[i, i + 4] = 1\n",
        "        self.kf.measurementMatrix = np.zeros((4, 8), np.float32)\n",
        "        for i in range(4):\n",
        "            self.kf.measurementMatrix[i, i] = 1\n",
        "        self.kf.processNoiseCov = np.eye(8, dtype=np.float32) * 1e-3\n",
        "        self.kf.measurementNoiseCov = np.eye(4, dtype=np.float32) * 1e-2\n",
        "        self.kf.statePost = np.array([[x], [y], [0], [0], [0], [0], [0], [0]], np.float32)\n",
        "\n",
        "    def predict(self):\n",
        "        pred = self.kf.predict()\n",
        "        return int(pred[0, 0]), int(pred[1, 0])\n",
        "\n",
        "    def correct(self, x, y):\n",
        "        self.kf.correct(np.array([[x], [y], [0], [0]], np.float32))\n",
        "\n",
        "#Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\").to(device).eval()\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "#Load video\n",
        "cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
        "assert cap.isOpened(), f\"Cannot open video: {VIDEO_PATH}\"\n",
        "W, H = int(cap.get(3)), int(cap.get(4))\n",
        "fps_in = cap.get(cv2.CAP_PROP_FPS) or 25\n",
        "out = cv2.VideoWriter(str(OUTPUT_PATH), cv2.VideoWriter_fourcc(*'mp4v'), fps_in, (W, H))\n",
        "\n",
        "#Initialize\n",
        "tracks = []\n",
        "frame_idx = 0\n",
        "start_time = time.time()\n",
        "ret, frame = cap.read()\n",
        "frame_idx += 1\n",
        "assert ret, \"Can't read the first frame\"\n",
        "img_tensor = transform(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = model([img_tensor])[0]\n",
        "\n",
        "boxes = preds[\"boxes\"].cpu().numpy()\n",
        "labels = preds[\"labels\"].cpu().numpy()\n",
        "scores = preds[\"scores\"].cpu().numpy()\n",
        "\n",
        "for box, cls, score in zip(boxes, labels, scores):\n",
        "    if score < CONF_TH:\n",
        "        continue\n",
        "    x1, y1, x2, y2 = map(int, box)\n",
        "    w, h = x2 - x1, y2 - y1\n",
        "    patch = frame[y1:y2, x1:x2]\n",
        "    if patch.size == 0:\n",
        "        continue\n",
        "    tracker = cv2.legacy.TrackerMOSSE_create()\n",
        "    tracker.init(frame, (x1, y1, w, h))\n",
        "    kalman = KalmanFilter(x1 + w // 2, y1 + h // 2)\n",
        "    hist = get_histogram(patch)\n",
        "    label = COCO.get(int(cls), f\"class{int(cls)}\")\n",
        "    tracks.append({\n",
        "        \"id\": str(uuid.uuid4())[:8],\n",
        "        \"tracker\": tracker,\n",
        "        \"label\": label,\n",
        "        \"kalman\": kalman,\n",
        "        \"hist\": hist,\n",
        "        \"lost\": 0\n",
        "    })\n",
        "    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "    cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
        "\n",
        "out.write(frame)\n",
        "\n",
        "#Track across video\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_idx += 1\n",
        "    new_tracks = []\n",
        "\n",
        "    for tr in tracks:\n",
        "        ok, bbox = tr[\"tracker\"].update(frame)\n",
        "        x, y, w, h = map(int, bbox)\n",
        "        x2, y2 = x + w, y + h\n",
        "\n",
        "        if not ok or w <= 0 or h <= 0:\n",
        "            tr[\"lost\"] += 1\n",
        "            pred_x, pred_y = tr[\"kalman\"].predict()\n",
        "            if tr[\"lost\"] <= MAX_LOST:\n",
        "                cv2.circle(frame, (pred_x, pred_y), 4, (255, 255, 0), -1)\n",
        "                new_tracks.append(tr)\n",
        "            continue\n",
        "\n",
        "        patch = frame[y:y+h, x:x+w]\n",
        "        if patch.size > 0:\n",
        "            hist_new = get_histogram(patch)\n",
        "            sim = compare_hist(tr[\"hist\"], hist_new)\n",
        "            if sim >= HIST_THRESH:\n",
        "                tr[\"kalman\"].correct(x + w // 2, y + h // 2)\n",
        "                tr[\"hist\"] = 0.8 * tr[\"hist\"] + 0.2 * hist_new\n",
        "                tr[\"lost\"] = 0\n",
        "            else:\n",
        "                tr[\"lost\"] += 1\n",
        "                if tr[\"lost\"] <= MAX_LOST:\n",
        "                    pred_x, pred_y = tr[\"kalman\"].predict()\n",
        "                    cv2.circle(frame, (pred_x, pred_y), 4, (255, 0, 0), -1)\n",
        "                    new_tracks.append(tr)\n",
        "                continue\n",
        "\n",
        "        cv2.rectangle(frame, (x, y), (x2, y2), (255, 0, 0), 2)\n",
        "        cv2.putText(frame, tr[\"label\"], (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
        "        new_tracks.append(tr)\n",
        "\n",
        "    tracks = new_tracks\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    fps = frame_idx / elapsed\n",
        "    cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "print(f\"âœ… Done. Output saved to: {OUTPUT_PATH}\")\n",
        "print(f\"ðŸ“ˆ Average FPS: {frame_idx / (time.time() - start_time):.2f}\")"
      ],
      "metadata": {
        "id": "qB1m9Iai_sFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CSRT_Tracker\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "#PATHS\n",
        "downloads   = Path.home() / \"Downloads\"\n",
        "VIDEO_PATH  = downloads / \"videos\" / \"car1.mp4\"\n",
        "OUTPUT_PATH = downloads / \"car1_CSRT_final_output.mp4\"\n",
        "\n",
        "#HYPERâ€‘PARAMETERS\n",
        "CONF_THRESHOLD  = 0.05\n",
        "IOU_THRESH      = 0.50     # skip duplicate tracks if IoU > this\n",
        "HIST_BINS       = (16,16,16)\n",
        "MAX_LOST_FRAMES = 60       # keep predicting this many missed frames\n",
        "\n",
        "def extract_hsv_hist(patch, bins=HIST_BINS):\n",
        "    hsv  = cv2.cvtColor(patch, cv2.COLOR_BGR2HSV)\n",
        "    hist = cv2.calcHist([hsv],[0,1,2],None,bins,[0,180,0,256,0,256])\n",
        "    cv2.normalize(hist, hist)\n",
        "    return hist.flatten()\n",
        "\n",
        "def iou(a, b):\n",
        "    xa,ya = max(a[0],b[0]), max(a[1],b[1])\n",
        "    xb,yb = min(a[2],b[2]), min(a[3],b[3])\n",
        "    inter = max(0, xb-xa) * max(0, yb-ya)\n",
        "    if inter == 0: return 0.0\n",
        "    area_a = (a[2]-a[0])*(a[3]-a[1])\n",
        "    area_b = (b[2]-b[0])*(b[3]-b[1])\n",
        "    return inter / (area_a + area_b - inter)\n",
        "class Track:\n",
        "    def __init__(self, bbox_xywh, label, frame):\n",
        "        self.tracker = cv2.TrackerCSRT_create()\n",
        "        self.tracker.init(frame, bbox_xywh)\n",
        "\n",
        "        x,y,w,h = map(int, bbox_xywh)\n",
        "        self.bbox  = [x,y,w,h]\n",
        "        self.label = label\n",
        "        self.lost  = 0\n",
        "\n",
        "        self.feature = extract_hsv_hist(frame[y:y+h, x:x+w])\n",
        "\n",
        "        # Kalman state (cx,cy,vx,vy)\n",
        "        cx, cy = x + w/2, y + h/2\n",
        "        self.kalman = cv2.KalmanFilter(4,2)\n",
        "        self.kalman.transitionMatrix   = np.array(\n",
        "            [[1,0,1,0],[0,1,0,1],[0,0,1,0],[0,0,0,1]], np.float32)\n",
        "        self.kalman.measurementMatrix  = np.eye(2,4, dtype=np.float32)\n",
        "        self.kalman.processNoiseCov    = np.eye(4, dtype=np.float32)*1e-2\n",
        "        self.kalman.measurementNoiseCov= np.eye(2, dtype=np.float32)*1e-1\n",
        "        self.kalman.statePre  = np.array([[cx],[cy],[0],[0]], np.float32)\n",
        "        self.kalman.statePost = self.kalman.statePre.copy()\n",
        "\n",
        "    def predict_center(self):\n",
        "        p = self.kalman.predict()\n",
        "        return float(p[0,0]), float(p[1,0])\n",
        "\n",
        "    def correct(self, cx, cy):\n",
        "        self.kalman.correct(np.array([[cx],[cy]], np.float32))\n",
        "\n",
        "    def box_xyxy(self):\n",
        "        x,y,w,h = self.bbox\n",
        "        return [x, y, x+w, y+h]\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
        "assert cap.isOpened(), f\"Cannot open {VIDEO_PATH}\"\n",
        "W, H   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps_in = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "out = cv2.VideoWriter(str(OUTPUT_PATH),\n",
        "                      cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "                      fps_in, (W,H))\n",
        "\n",
        "tracks      = []\n",
        "prev_gray   = None\n",
        "prev_pts    = None\n",
        "frame_idx   = 0\n",
        "start_time  = time.perf_counter()\n",
        "\n",
        "feature_params = dict(maxCorners=200, qualityLevel=0.3,\n",
        "                      minDistance=7, blockSize=7)\n",
        "lk_params = dict(winSize=(15,15), maxLevel=2,\n",
        "                 criteria=(cv2.TERM_CRITERIA_EPS|cv2.TERM_CRITERIA_COUNT,\n",
        "                           10, 0.03))\n",
        "\n",
        "#MAIN LOOP\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_idx += 1\n",
        "    t0 = time.perf_counter()\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    #First frame: detect & spawn CSRT trackers\n",
        "    if frame_idx == 1:\n",
        "        det = model.predict(frame, conf=CONF_THRESHOLD,\n",
        "                            classes=[0,3], verbose=False)[0]\n",
        "        for box, cls in zip(det.boxes.xyxy.cpu().numpy().astype(int),\n",
        "                            det.boxes.cls.cpu().numpy().astype(int)):\n",
        "            if any(tr.label == model.names[int(cls)] and\n",
        "                   iou(box, tr.box_xyxy()) > IOU_THRESH for tr in tracks):\n",
        "                continue\n",
        "            x1,y1,x2,y2 = box\n",
        "            tracks.append(Track((x1,y1,x2-x1,y2-y1),\n",
        "                                model.names[int(cls)], frame))\n",
        "        prev_gray = gray.copy()\n",
        "        prev_pts  = cv2.goodFeaturesToTrack(prev_gray, mask=None,\n",
        "                                            **feature_params)\n",
        "    #Subsequent frames\n",
        "    else:\n",
        "        # 1) global motion compensation\n",
        "        curr_pts, st, _ = cv2.calcOpticalFlowPyrLK(prev_gray, gray,\n",
        "                                                  prev_pts, None, **lk_params)\n",
        "        good_prev = prev_pts[st.flatten()==1]\n",
        "        good_curr = curr_pts[st.flatten()==1]\n",
        "        M = np.eye(2,3, dtype=np.float32)\n",
        "        if len(good_prev) >= 6:\n",
        "            M,_ = cv2.estimateAffinePartial2D(good_prev, good_curr,\n",
        "                                              method=cv2.RANSAC)\n",
        "        stab = cv2.warpAffine(frame, M, (W,H))\n",
        "\n",
        "        # 2) update each track\n",
        "        new_tracks = []\n",
        "        for tr in tracks:\n",
        "            pcx, pcy = tr.predict_center()\n",
        "            ok, bbox = tr.tracker.update(stab)\n",
        "            if ok:\n",
        "                tr.lost = 0\n",
        "                tr.bbox = bbox\n",
        "                x,y,w,h = map(int,bbox)\n",
        "                tr.correct(x + w/2, y + h/2)\n",
        "            else:\n",
        "                tr.lost += 1\n",
        "                if tr.lost > MAX_LOST_FRAMES:\n",
        "                    continue  # give up on this track\n",
        "                # fabricate a bbox around predicted center\n",
        "                w,h = tr.bbox[2], tr.bbox[3]\n",
        "                x,y = int(pcx - w/2), int(pcy - h/2)\n",
        "                tr.bbox = [x,y,w,h]\n",
        "\n",
        "            # draw back onto original (unstabilised) frame\n",
        "            x,y,w,h = tr.bbox\n",
        "            pts  = np.array([[x,y], [x+w,y+h]], np.float32).reshape(-1,1,2)\n",
        "            invM = cv2.invertAffineTransform(M)\n",
        "            ox1,oy1, ox2,oy2 = cv2.transform(pts, invM).reshape(-1,2).flatten()\n",
        "            cv2.rectangle(frame, (int(ox1),int(oy1)),\n",
        "                          (int(ox2),int(oy2)), (0,255,0), 2)\n",
        "            cv2.putText(frame, tr.label, (int(ox1),int(oy1)-10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
        "            cv2.circle(frame, (int(pcx),int(pcy)), 4, (0,0,255), -1)\n",
        "            new_tracks.append(tr)\n",
        "        tracks = new_tracks\n",
        "\n",
        "    #FPS overlay\n",
        "    fps = 1.0 / (time.perf_counter() - t0)\n",
        "    cv2.putText(frame, f\"FPS: {fps:.1f}\",\n",
        "                (10,30), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.9, (0,255,255), 2)\n",
        "\n",
        "    out.write(frame)\n",
        "    prev_gray = gray.copy()\n",
        "    prev_pts  = cv2.goodFeaturesToTrack(prev_gray, mask=None,\n",
        "                                        **feature_params)\n",
        "\n",
        "total = time.perf_counter() - start_time\n",
        "print(f\"Processed {frame_idx} frames in {total:.2f}s â€” \"\n",
        "      f\"Avg FPS: {frame_idx/total:.2f}\")\n",
        "print(f\"Output saved to: {OUTPUT_PATH}\")\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "UtCZThCAChMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KCF_tracker\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import models, transforms\n",
        "from pathlib import Path\n",
        "import uuid\n",
        "import time\n",
        "#PATHS\n",
        "downloads = Path.home() / \"Downloads\"\n",
        "VIDEO_PATH = downloads / \"videos\" / \"car1.mp4\"\n",
        "OUTPUT_PATH = downloads / \"car1_kcf_final_output.mp4\"\n",
        "\n",
        "#CONSTANTS\n",
        "CONF_TH = 0.6\n",
        "MAX_LOST = 180\n",
        "HIST_THRESHOLD = 0.5\n",
        "HIST_BINS = (16, 16, 16)\n",
        "\n",
        "#COCO LABELS\n",
        "COCO = {i: name for i, name in enumerate([\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', '', 'stop sign',\n",
        "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', '', 'backpack', 'umbrella', '', '',\n",
        "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
        "    'tennis racket', 'bottle', '', 'wine glass', 'cup', 'fork', 'knife',\n",
        "    'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli',\n",
        "    'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "    'potted plant', 'bed', '', 'dining table', '', '', 'toilet', '', 'tv',\n",
        "    'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
        "    'oven', 'toaster', 'sink', 'refrigerator', '', 'book', 'clock', 'vase',\n",
        "    'scissors', 'teddy bear', 'hair drier', 'toothbrush'])}\n",
        "\n",
        "def get_histogram(patch, bins=HIST_BINS):\n",
        "    hsv = cv2.cvtColor(patch, cv2.COLOR_BGR2HSV)\n",
        "    hist = cv2.calcHist([hsv], [0, 1, 2], None, bins, [0, 180, 0, 256, 0, 256])\n",
        "    cv2.normalize(hist, hist)\n",
        "    return hist.flatten()\n",
        "\n",
        "def compare_hist(hist1, hist2):\n",
        "    return cv2.compareHist(hist1.astype(np.float32), hist2.astype(np.float32), cv2.HISTCMP_CORREL)\n",
        "\n",
        "class StrongKalmanFilter:\n",
        "    def __init__(self, x, y):\n",
        "        self.kf = cv2.KalmanFilter(8, 4)\n",
        "        self.kf.transitionMatrix = np.eye(8, dtype=np.float32)\n",
        "        for i in range(4):\n",
        "            self.kf.transitionMatrix[i, i+4] = 1.0\n",
        "        self.kf.measurementMatrix = np.zeros((4, 8), np.float32)\n",
        "        for i in range(4):\n",
        "            self.kf.measurementMatrix[i, i] = 1.0\n",
        "        self.kf.processNoiseCov = np.eye(8, dtype=np.float32) * 1e-2\n",
        "        self.kf.measurementNoiseCov = np.eye(4, dtype=np.float32) * 1e-1\n",
        "        self.kf.errorCovPost = np.eye(8, dtype=np.float32)\n",
        "        self.kf.statePost = np.array([[x], [y], [0], [0], [0], [0], [0], [0]], dtype=np.float32)\n",
        "\n",
        "    def predict(self):\n",
        "        pred = self.kf.predict()\n",
        "        return int(pred[0]), int(pred[1])\n",
        "\n",
        "    def correct(self, x, y):\n",
        "        self.kf.correct(np.array([[x], [y], [0], [0]], dtype=np.float32))\n",
        "\n",
        "#MODEL\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "model.to(device).eval()\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "#VIDEO\n",
        "cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
        "assert cap.isOpened(), f\"Could not open {VIDEO_PATH}\"\n",
        "W, H = int(cap.get(3)), int(cap.get(4))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS) or 25\n",
        "out = cv2.VideoWriter(str(OUTPUT_PATH), cv2.VideoWriter_fourcc(*'mp4v'), fps, (W, H))\n",
        "\n",
        "tracks = []\n",
        "frame_idx = 0\n",
        "t0 = prev = time.time()\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_idx += 1\n",
        "\n",
        "    if frame_idx == 1:\n",
        "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        img_tensor = transform(rgb).to(device)\n",
        "        with torch.no_grad():\n",
        "            preds = model([img_tensor])[0]\n",
        "\n",
        "        boxes = preds['boxes'].cpu().numpy()\n",
        "        labels_raw = preds['labels'].cpu().numpy()\n",
        "        scores = preds['scores'].cpu().numpy()\n",
        "        keep = scores >= CONF_TH\n",
        "        boxes, labels_raw = boxes[keep], labels_raw[keep]\n",
        "\n",
        "        for box, cls_id in zip(boxes, labels_raw):\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            w, h = x2 - x1, y2 - y1\n",
        "            patch = frame[y1:y2, x1:x2]\n",
        "            if patch.size == 0:\n",
        "                continue\n",
        "            tracker = cv2.TrackerKCF_create()\n",
        "            tracker.init(frame, (x1, y1, w, h))\n",
        "            hist = get_histogram(patch)\n",
        "            kalman = StrongKalmanFilter(x1 + w//2, y1 + h//2)\n",
        "            tracks.append({\n",
        "                \"id\": str(uuid.uuid4())[:8],\n",
        "                \"tracker\": tracker,\n",
        "                \"label\": COCO.get(int(cls_id), f\"class{cls_id}\"),\n",
        "                \"hist\": hist,\n",
        "                \"lost\": 0,\n",
        "                \"kalman\": kalman\n",
        "            })\n",
        "    else:\n",
        "        new_tracks = []\n",
        "        for tr in tracks:\n",
        "            ok, bbox = tr[\"tracker\"].update(frame)\n",
        "            x, y, w, h = map(int, bbox)\n",
        "            x2, y2 = x + w, y + h\n",
        "\n",
        "            if not ok or x < 0 or y < 0 or x2 > W or y2 > H:\n",
        "                tr[\"lost\"] += 1\n",
        "                pred_x, pred_y = tr[\"kalman\"].predict()\n",
        "                if tr[\"lost\"] <= MAX_LOST:\n",
        "                    cv2.circle(frame, (pred_x, pred_y), 4, (0, 0, 255), -1)\n",
        "                    new_tracks.append(tr)\n",
        "                continue\n",
        "\n",
        "            patch = frame[y:y+h, x:x+w]\n",
        "            if patch.size > 0:\n",
        "                hist_new = get_histogram(patch)\n",
        "                similarity = compare_hist(tr[\"hist\"], hist_new)\n",
        "                if similarity < HIST_THRESHOLD:\n",
        "                    tr[\"lost\"] += 1\n",
        "                    if tr[\"lost\"] <= MAX_LOST:\n",
        "                        new_tracks.append(tr)\n",
        "                    continue\n",
        "                else:\n",
        "                    tr[\"lost\"] = 0\n",
        "                    tr[\"hist\"] = 0.8 * tr[\"hist\"] + 0.2 * hist_new\n",
        "                    tr[\"kalman\"].correct(x + w//2, y + h//2)\n",
        "\n",
        "            cv2.rectangle(frame, (x, y), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, tr[\"label\"], (x, y - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "            new_tracks.append(tr)\n",
        "\n",
        "        tracks = new_tracks\n",
        "\n",
        "    #FPS Overlay\n",
        "    now = time.time()\n",
        "    fps_now = 1.0 / (now - prev + 1e-6)\n",
        "    prev = now\n",
        "    cv2.putText(frame, f\"FPS: {fps_now:.1f}\", (10, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "total_time = time.time() - t0\n",
        "avg_fps = frame_idx / total_time\n",
        "print(f\"âœ… Saved to: {OUTPUT_PATH}\")\n",
        "print(f\"Average FPS: {avg_fps:.2f}\")\n"
      ],
      "metadata": {
        "id": "Qxlyl9YCGCfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KJF_Tracker_Main\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from collections import deque\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# ----------- COCO classes -----------\n",
        "CLASSES = {\n",
        "    0:\"background\", 1:\"person\", 2:\"bicycle\", 3:\"car\", 4:\"motorcycle\", 5:\"airplane\",\n",
        "    6:\"bus\",7:\"train\",8:\"truck\",9:\"boat\",10:\"trafficlight\",11:\"firehydrant\",\n",
        "    13:\"bench\",14:\"bird\",15:\"cat\",16:\"dog\",17:\"horse\",18:\"sheep\",19:\"cow\",\n",
        "    20:\"elephant\",21:\"bear\",22:\"zebra\",23:\"giraffe\",24:\"backpack\",25:\"umbrella\",\n",
        "    27:\"handbag\",28:\"tie\",31:\"snowboard\",32:\"sportsball\",33:\"kite\",34:\"baseballbat\",\n",
        "    35:\"skis\",36:\"skateboard\",37:\"surfboard\",38:\"tennisracket\",39:\"bottle\",\n",
        "    40:\"wineglass\",41:\"cup\",42:\"fork\",43:\"knife\",44:\"spoon\",45:\"bowl\",46:\"banana\",\n",
        "    47:\"apple\",48:\"sandwich\",49:\"orange\",50:\"broccoli\",51:\"carrot\",52:\"hotdog\",\n",
        "    53:\"pizza\",54:\"donut\",55:\"cake\",56:\"chair\",57:\"couch\",58:\"pottedplant\",59:\"bed\",\n",
        "    60:\"diningtable\",61:\"toilet\",62:\"tv\",63:\"laptop\",64:\"mouse\",65:\"remote\",\n",
        "    66:\"keyboard\",67:\"cellphone\",68:\"microwave\",69:\"oven\",70:\"toaster\",71:\"sink\",\n",
        "    72:\"refrigerator\",73:\"book\",74:\"clock\",75:\"vase\",76:\"scissors\",77:\"teddybear\",\n",
        "    78:\"hairdrier\",79:\"toothbrush\"\n",
        "}\n",
        "\n",
        "# ----------- Settings -----------\n",
        "VIDEO_PATH = str(Path.home()/\"Downloads\"/\"videos\"/\"person4.mp4\")\n",
        "OUTPUT_PATH = str(Path.home()/\"Downloads\"/\"person4_frcnn_kjf_output.mp4\")\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "CONF_TH = 0.5         # Detection confidence threshold\n",
        "MIN_LK_POINTS = 3     # Minimum good LK points to trust tracking\n",
        "REINIT_KP = 2        # Frames before reinit keypoints\n",
        "DROP_LIMIT = 500      # Drop track if lost too long\n",
        "GRID = (4, 4)\n",
        "MAX_CORNERS = [4000, 3000]\n",
        "MIN_DIST = [3, 3]\n",
        "\n",
        "# ----------- Kalman Filter Class -----------\n",
        "class KalmanBox:\n",
        "    count = 0\n",
        "    def __init__(self, box):\n",
        "        self.id = KalmanBox.count; KalmanBox.count += 1\n",
        "        self.lost = 0\n",
        "        self.kf = cv2.KalmanFilter(10, 4)\n",
        "        M = np.zeros((4,10), np.float32)\n",
        "        M[[0,1,2,3],[0,1,6,7]] = 1\n",
        "        self.kf.measurementMatrix = M\n",
        "        dt, dt2 = 1/30.0, 0.5*(1/30.0)**2\n",
        "        T = np.eye(10, dtype=np.float32)\n",
        "        T[0,2],T[0,4]=dt,dt2; T[1,3],T[1,5]=dt,dt2\n",
        "        T[2,4],T[3,5]=dt,dt;   T[6,8],T[7,9]=dt,dt\n",
        "        self.kf.transitionMatrix    = T\n",
        "        self.kf.processNoiseCov     = np.eye(10, dtype=np.float32)*0.01\n",
        "        self.kf.measurementNoiseCov = np.eye(4,  dtype=np.float32)*0.02\n",
        "        x1,y1,x2,y2 = box\n",
        "        cx,cy = (x1+x2)/2,(y1+y2)/2\n",
        "        w,h   = x2-x1,    y2-y1\n",
        "        self.kf.statePost = np.array([cx,cy,0,0,0,0,w,h,0,0],np.float32).reshape(-1,1)\n",
        "        self.last = self._to_box(self.kf.statePost.flatten())\n",
        "    def _to_box(self, s):\n",
        "        cx,cy,w,h = s[0],s[1],s[6],s[7]\n",
        "        return (int(cx-w/2), int(cy-h/2), int(cx+w/2), int(cy+h/2))\n",
        "    def predict(self):\n",
        "        s = self.kf.predict().flatten()\n",
        "        self.lost += 1\n",
        "        self.last = self._to_box(s)\n",
        "        return self.last\n",
        "    def update(self, box):\n",
        "        x1,y1,x2,y2 = box\n",
        "        cx,cy = (x1+x2)/2,(y1+y2)/2\n",
        "        w,h   = x2-x1,    y2-y1\n",
        "        meas  = np.array([cx,cy,w,h],np.float32).reshape(-1,1)\n",
        "        self.kf.correct(meas)\n",
        "        self.lost = 0\n",
        "        self.last = self._to_box(self.kf.statePost.flatten())\n",
        "\n",
        "# ----------- Helper Functions -----------\n",
        "def get_distributed_kps(gray, grid, max_c, min_d):\n",
        "    h,w=gray.shape; gh,gw=grid; pts=[]\n",
        "    for i in range(gh):\n",
        "        for j in range(gw):\n",
        "            roi=gray[i*h//gh:(i+1)*h//gh, j*w//gw:(j+1)*w//gw]\n",
        "            c=cv2.goodFeaturesToTrack(roi,\n",
        "                maxCorners=max_c//(gh*gw),\n",
        "                qualityLevel=0.05, minDistance=min_d)\n",
        "            if c is not None:\n",
        "                for [[x,y]] in c:\n",
        "                    pts.append([x+j*w//gw, y+i*h//gh])\n",
        "    return np.array(pts,np.float32).reshape(-1,1,2) if pts else None\n",
        "\n",
        "def filter_pts(nw,old,max_m=25,thr=2.08,sk=100):\n",
        "    if len(nw)==0 or len(old)==0: return np.array([],bool)\n",
        "    mv=np.linalg.norm(nw-old,axis=1)<max_m\n",
        "    d =np.linalg.norm(nw-nw.mean(0),axis=1)\n",
        "    cl=d<=thr*np.mean(np.delete(d,0))\n",
        "    sp=np.linalg.norm(nw-old,axis=1)\n",
        "    sd=(sp>=sp.mean()-sk*sp.std())&(sp<=sp.mean()+sk*sp.std())\n",
        "    return mv & cl & sd\n",
        "\n",
        "# ----------- Main Script -----------\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "ret, frame_bgr = cap.read()\n",
        "if not ret:\n",
        "    raise RuntimeError(\"Cannot read video\")\n",
        "\n",
        "frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "model = models.detection.fasterrcnn_resnet50_fpn(\n",
        "    weights=models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        ").to(DEVICE).eval()\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "img_t = transform(frame_rgb).to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = model([img_t])[0]\n",
        "\n",
        "boxes  = preds[\"boxes\"].cpu().numpy().astype(int)\n",
        "labels = preds[\"labels\"].cpu().numpy()\n",
        "scores = preds[\"scores\"].cpu().numpy()\n",
        "\n",
        "keep = scores >= CONF_TH\n",
        "boxes, labels, scores = boxes[keep], labels[keep], scores[keep]\n",
        "\n",
        "h0, w0 = frame_bgr.shape[:2]\n",
        "out = cv2.VideoWriter(\n",
        "    OUTPUT_PATH,\n",
        "    cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "    cap.get(cv2.CAP_PROP_FPS),\n",
        "    (w0, h0)\n",
        ")\n",
        "# Draw detection boxes on first frame\n",
        "for (x1,y1,x2,y2), cls_id, sc in zip(boxes, labels, scores):\n",
        "    name = CLASSES.get(int(cls_id), str(int(cls_id)))\n",
        "    label = f\"{name} {sc:.2f}\"\n",
        "    cv2.rectangle(frame_bgr, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "    (tw,th),_ = cv2.getTextSize(label,\n",
        "                  cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
        "    cv2.rectangle(frame_bgr,\n",
        "                  (x1,y1-th-8),(x1+tw,y1),\n",
        "                  (0,255,0), -1)\n",
        "    cv2.putText(frame_bgr, label,\n",
        "                (x1,y1-4),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.7, (0,0,0), 2)\n",
        "out.write(frame_bgr)\n",
        "\n",
        "gray0 = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
        "trackers = []\n",
        "for i, (b, cid) in enumerate(zip(boxes, labels)):\n",
        "    kf = KalmanBox(tuple(b))\n",
        "    x1,y1,x2,y2 = b\n",
        "    kp = get_distributed_kps(\n",
        "        gray0[y1:y2, x1:x2],\n",
        "        GRID,\n",
        "        MAX_CORNERS[min(i,len(MAX_CORNERS)-1)],\n",
        "        MIN_DIST   [min(i,len(MIN_DIST)-1)]\n",
        "    )\n",
        "    if kp is not None and len(kp) >= MIN_LK_POINTS:\n",
        "        kp[:,:,0] += x1; kp[:,:,1] += y1\n",
        "        history = deque([kp.reshape(-1,2)], maxlen=REINIT_KP)\n",
        "    else:\n",
        "        kp, history = None, deque([], maxlen=REINIT_KP)\n",
        "    trackers.append({\n",
        "        'kf': kf,\n",
        "        'p0': kp,\n",
        "        'history': history,\n",
        "        'cls': int(cid),\n",
        "        'lost': 0,\n",
        "        'age': 0   # age field for track memory\n",
        "    })\n",
        "\n",
        "old_gray = gray0.copy()\n",
        "frame_count = 1\n",
        "start_time = time.time()\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret: break\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    frame_count += 1\n",
        "\n",
        "    for obj in trackers[:]:\n",
        "        kf = obj['kf']\n",
        "        x1,y1,x2,y2 = kf.last\n",
        "        obj['age'] += 1\n",
        "\n",
        "        # LK Optical Flow\n",
        "        if obj['p0'] is not None:\n",
        "            p1, st, _ = cv2.calcOpticalFlowPyrLK(\n",
        "                old_gray, gray, obj['p0'], None,\n",
        "                winSize=(15,15), maxLevel=2,\n",
        "                criteria=(cv2.TERM_CRITERIA_EPS|\n",
        "                          cv2.TERM_CRITERIA_COUNT,10,0.03)\n",
        "            )\n",
        "        else:\n",
        "            p1, st = None, 0\n",
        "\n",
        "        # If enough points: update box tightly to hull\n",
        "        if p1 is not None and st.sum() >= MIN_LK_POINTS:\n",
        "            newp = p1[st==1].reshape(-1,2)\n",
        "            oldp = obj['p0'][st==1].reshape(-1,2)\n",
        "            pts  = newp[filter_pts(newp,oldp)]\n",
        "            if len(pts) >= MIN_LK_POINTS:\n",
        "                obj['history'].append(pts)\n",
        "                allpts = np.vstack(obj['history'])\n",
        "                hull   = cv2.convexHull(allpts.astype(np.float32))\n",
        "                bx,by,bw,bh = cv2.boundingRect(hull)\n",
        "                # Update Kalman from the hull-based box\n",
        "                kf.update((bx,by,bx+bw,by+bh))\n",
        "                x1, y1, x2, y2 = bx, by, bx+bw, by+bh\n",
        "                obj['p0'] = pts.reshape(-1,1,2)\n",
        "                obj['lost'] = 0\n",
        "            else:\n",
        "                x1,y1,x2,y2 = kf.predict()\n",
        "                obj['p0'], obj['lost'] = None, obj['lost']+1\n",
        "        else:\n",
        "            x1,y1,x2,y2 = kf.predict()\n",
        "            obj['p0'], obj['lost'] = None, obj['lost']+1\n",
        "\n",
        "        # Re-init keypoints after several lost frames\n",
        "        if obj['lost'] == REINIT_KP:\n",
        "            xx1,yy1 = max(0,x1), max(0,y1)\n",
        "            xx2,yy2 = min(w0,x2), min(h0,y2)\n",
        "            newkp = get_distributed_kps(\n",
        "                gray[yy1:yy2,xx1:xx2],\n",
        "                GRID, 2000, 5\n",
        "            )\n",
        "            if newkp is not None and len(newkp) >= MIN_LK_POINTS:\n",
        "                newkp[:,:,0]+=xx1; newkp[:,:,1]+=yy1\n",
        "                obj['history'].clear()\n",
        "                obj['history'].append(newkp.reshape(-1,2))\n",
        "                obj['p0'], obj['lost'] = newkp, 0\n",
        "\n",
        "        # Drop track if lost for too long\n",
        "        if obj['lost'] >= DROP_LIMIT:\n",
        "            trackers.remove(obj)\n",
        "            continue\n",
        "\n",
        "        # Draw TIGHT bounding box always around the object\n",
        "        color = (0,255,0)\n",
        "        if obj['p0'] is not None and len(obj['p0']) >= MIN_LK_POINTS:\n",
        "            pts = obj['p0'].reshape(-1,2)\n",
        "            hull = cv2.convexHull(pts.astype(np.float32))\n",
        "            bx, by, bw, bh = cv2.boundingRect(hull)\n",
        "            cv2.rectangle(frame, (bx,by), (bx+bw,by+bh), color, 2)\n",
        "            label = CLASSES.get(obj['cls'], str(obj['cls']))\n",
        "            cv2.putText(frame, f\"{label} #{obj['kf'].id} age:{obj['age']}\", (bx, by-6),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "            for pt in obj['p0']:\n",
        "                cv2.circle(frame, tuple(pt[0].astype(int)), 2, (0,0,255), -1)\n",
        "        else:\n",
        "            # Draw predicted box if no points\n",
        "            x1, y1, x2, y2 = kf.last\n",
        "            cv2.rectangle(frame, (x1,y1), (x2,y2), (255,0,0), 2)\n",
        "            label = CLASSES.get(obj['cls'], str(obj['cls']))\n",
        "            cv2.putText(frame, f\"{label} #{obj['kf'].id} age:{obj['age']}\", (x1, y1-6),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,0,0), 2)\n",
        "\n",
        "    out.write(frame)\n",
        "    old_gray = gray.copy()\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "fps = frame_count / total_time\n",
        "print(f\"âœ… Done â€“ saved to {OUTPUT_PATH}\")\n",
        "print(f\"Average FPS: {fps:.2f}\")"
      ],
      "metadata": {
        "id": "e9kTzdZnITnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KJF_unsuccessful\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from torchvision.models import ResNet18_Weights\n",
        "from collections import deque\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "#COCO classes\n",
        "CLASSES = {\n",
        "    0:\"background\", 1:\"person\", 2:\"bicycle\", 3:\"car\", 4:\"motorcycle\", 5:\"airplane\",\n",
        "    6:\"bus\",7:\"train\",8:\"truck\",9:\"boat\",10:\"trafficlight\",11:\"firehydrant\",\n",
        "    13:\"bench\",14:\"bird\",15:\"cat\",16:\"dog\",17:\"horse\",18:\"sheep\",19:\"cow\",\n",
        "    20:\"elephant\",21:\"bear\",22:\"zebra\",23:\"giraffe\",24:\"backpack\",25:\"umbrella\",\n",
        "    27:\"handbag\",28:\"tie\",31:\"snowboard\",32:\"sportsball\",33:\"kite\",34:\"baseballbat\",\n",
        "    35:\"skis\",36:\"skateboard\",37:\"surfboard\",38:\"tennisracket\",39:\"bottle\",\n",
        "    40:\"wineglass\",41:\"cup\",42:\"fork\",43:\"knife\",44:\"spoon\",45:\"bowl\",46:\"banana\",\n",
        "    47:\"apple\",48:\"sandwich\",49:\"orange\",50:\"broccoli\",51:\"carrot\",52:\"hotdog\",\n",
        "    53:\"pizza\",54:\"donut\",55:\"cake\",56:\"chair\",57:\"couch\",58:\"pottedplant\",59:\"bed\",\n",
        "    60:\"diningtable\",61:\"toilet\",62:\"tv\",63:\"laptop\",64:\"mouse\",65:\"remote\",\n",
        "    66:\"keyboard\",67:\"cellphone\",68:\"microwave\",69:\"oven\",70:\"toaster\",71:\"sink\",\n",
        "    72:\"refrigerator\",73:\"book\",74:\"clock\",75:\"vase\",76:\"scissors\",77:\"teddybear\",\n",
        "    78:\"hairdrier\",79:\"toothbrush\"\n",
        "}\n",
        "\n",
        "#Settings\n",
        "VIDEO_PATH = str(Path.home()/\"Downloads\"/\"videos\"/\"person2.mp4\")\n",
        "OUTPUT_PATH = str(Path.home()/\"Downloads\"/\"person2_frcnn_kjf_output.mp4\")\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "CONF_TH = 0.5\n",
        "MIN_LK_POINTS = 3\n",
        "REINIT_KP = 2\n",
        "DROP_LIMIT = 500\n",
        "GRID = (4, 4)\n",
        "MAX_CORNERS = [4000, 3000]\n",
        "MIN_DIST = [3, 3]\n",
        "REID_MAX_AGE = 60\n",
        "REID_SIM_THRESH = 0.30\n",
        "HOG_SIM_THRESH = 0.6\n",
        "DEEP_SIM_THRESH = 0.85  # Cosine similarity for deep features (higher = more similar)\n",
        "\n",
        "#Deep feature extractor (ResNet-18, remove classifier)\n",
        "deep_model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "deep_model.fc = torch.nn.Identity()  # Remove the last layer\n",
        "deep_model = deep_model.eval().to(DEVICE)\n",
        "deep_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((128, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
        "])\n",
        "def extract_deep_feature(img, box):\n",
        "    x1, y1, x2, y2 = [int(e) for e in box]\n",
        "    x1 = max(0, x1); y1 = max(0, y1); x2 = min(img.shape[1]-1, x2); y2 = min(img.shape[0]-1, y2)\n",
        "    patch = img[y1:y2, x1:x2]\n",
        "    if patch.size == 0 or patch.shape[0] < 16 or patch.shape[1] < 16:\n",
        "        return None\n",
        "    try:\n",
        "        inp = deep_transform(patch).unsqueeze(0).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            feat = deep_model(inp).cpu().numpy().flatten()\n",
        "        feat = feat / (np.linalg.norm(feat) + 1e-8)\n",
        "        return feat\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def deep_sim(f1, f2):\n",
        "    if f1 is None or f2 is None or len(f1) != len(f2): return 0\n",
        "    return float(np.dot(f1, f2) / (np.linalg.norm(f1)+1e-8) / (np.linalg.norm(f2)+1e-8))\n",
        "\n",
        "#Color histogram\n",
        "def color_hist(img, box):\n",
        "    x1, y1, x2, y2 = [int(e) for e in box]\n",
        "    x1 = max(0, x1); y1 = max(0, y1); x2 = min(img.shape[1]-1, x2); y2 = min(img.shape[0]-1, y2)\n",
        "    patch = img[y1:y2, x1:x2]\n",
        "    if patch.size == 0 or patch.shape[0] < 5 or patch.shape[1] < 5:\n",
        "        return None\n",
        "    hsv = cv2.cvtColor(patch, cv2.COLOR_BGR2HSV)\n",
        "    hist = cv2.calcHist([hsv], [0,1,2], None, [8,8,8], [0,180,0,256,0,256])\n",
        "    cv2.normalize(hist, hist)\n",
        "    return hist.flatten()\n",
        "def hist_sim(h1, h2):\n",
        "    if h1 is None or h2 is None: return 1.0\n",
        "    return cv2.compareHist(h1.reshape(8,8,8), h2.reshape(8,8,8), cv2.HISTCMP_BHATTACHARYYA)\n",
        "\n",
        "#HOG descriptor\n",
        "def compute_hog(img, box):\n",
        "    x1, y1, x2, y2 = [int(e) for e in box]\n",
        "    x1 = max(0, x1); y1 = max(0, y1); x2 = min(img.shape[1]-1, x2); y2 = min(img.shape[0]-1, y2)\n",
        "    patch = img[y1:y2, x1:x2]\n",
        "    WIN_W, WIN_H = 64, 128\n",
        "    if patch.size == 0 or patch.shape[0] < 16 or patch.shape[1] < 16:\n",
        "        return None\n",
        "    gray = cv2.cvtColor(patch, cv2.COLOR_BGR2GRAY)\n",
        "    resized = cv2.resize(gray, (WIN_W, WIN_H))\n",
        "    hog = cv2.HOGDescriptor(_winSize=(WIN_W, WIN_H),\n",
        "                            _blockSize=(16,16),\n",
        "                            _blockStride=(8,8),\n",
        "                            _cellSize=(8,8),\n",
        "                            _nbins=9)\n",
        "    desc = hog.compute(resized)\n",
        "    return desc.flatten() if desc is not None else None\n",
        "def hog_sim(h1, h2):\n",
        "    if h1 is None or h2 is None or len(h1) != len(h2): return 0\n",
        "    h1 = h1 / (np.linalg.norm(h1) + 1e-8)\n",
        "    h2 = h2 / (np.linalg.norm(h2) + 1e-8)\n",
        "    return float(np.dot(h1, h2))\n",
        "\n",
        "#Kalman Filter Class\n",
        "class KalmanBox:\n",
        "    count = 0\n",
        "    def __init__(self, box, hist, hog_desc, deep_feat):\n",
        "        self.id = KalmanBox.count; KalmanBox.count += 1\n",
        "        self.lost = 0\n",
        "        self.kf = cv2.KalmanFilter(10, 4)\n",
        "        M = np.zeros((4,10), np.float32)\n",
        "        M[[0,1,2,3],[0,1,6,7]] = 1\n",
        "        self.kf.measurementMatrix = M\n",
        "        dt, dt2 = 1/30.0, 0.5*(1/30.0)**2\n",
        "        T = np.eye(10, dtype=np.float32)\n",
        "        T[0,2],T[0,4]=dt,dt2; T[1,3],T[1,5]=dt,dt2\n",
        "        T[2,4],T[3,5]=dt,dt;   T[6,8],T[7,9]=dt,dt\n",
        "        self.kf.transitionMatrix    = T\n",
        "        self.kf.processNoiseCov     = np.eye(10, dtype=np.float32)*0.01\n",
        "        self.kf.measurementNoiseCov = np.eye(4,  dtype=np.float32)*0.02\n",
        "        x1,y1,x2,y2 = box\n",
        "        cx,cy = (x1+x2)/2,(y1+y2)/2\n",
        "        w,h   = x2-x1,    y2-y1\n",
        "        self.kf.statePost = np.array([cx,cy,0,0,0,0,w,h,0,0],np.float32).reshape(-1,1)\n",
        "        self.last = self._to_box(self.kf.statePost.flatten())\n",
        "        # Memory for appearance\n",
        "        self.hist = hist\n",
        "        self.hog_desc = hog_desc\n",
        "        self.deep_feat = deep_feat\n",
        "        self.memory = [hist] if hist is not None else []\n",
        "        self.memory_hog = [hog_desc] if hog_desc is not None else []\n",
        "        self.memory_deep = [deep_feat] if deep_feat is not None else []\n",
        "        self.max_mem = 5\n",
        "        self.age = 0\n",
        "        self.last_frame = 0\n",
        "        self.last_box = box\n",
        "\n",
        "    def _to_box(self, s):\n",
        "        cx,cy,w,h = s[0],s[1],s[6],s[7]\n",
        "        return (int(cx-w/2), int(cy-h/2), int(cx+w/2), int(cy+h/2))\n",
        "    def predict(self):\n",
        "        s = self.kf.predict().flatten()\n",
        "        self.lost += 1\n",
        "        self.last = self._to_box(s)\n",
        "        return self.last\n",
        "    def update(self, box, hist=None, hog_desc=None, deep_feat=None):\n",
        "        x1,y1,x2,y2 = box\n",
        "        cx,cy = (x1+x2)/2,(y1+y2)/2\n",
        "        w,h   = x2-x1,    y2-y1\n",
        "        meas  = np.array([cx,cy,w,h],np.float32).reshape(-1,1)\n",
        "        self.kf.correct(meas)\n",
        "        self.lost = 0\n",
        "        self.last = self._to_box(self.kf.statePost.flatten())\n",
        "        self.last_box = box\n",
        "        if hist is not None:\n",
        "            self.hist = hist\n",
        "            self.memory.append(hist)\n",
        "            if len(self.memory) > self.max_mem:\n",
        "                self.memory.pop(0)\n",
        "        if hog_desc is not None:\n",
        "            self.hog_desc = hog_desc\n",
        "            self.memory_hog.append(hog_desc)\n",
        "            if len(self.memory_hog) > self.max_mem:\n",
        "                self.memory_hog.pop(0)\n",
        "        if deep_feat is not None:\n",
        "            self.deep_feat = deep_feat\n",
        "            self.memory_deep.append(deep_feat)\n",
        "            if len(self.memory_deep) > self.max_mem:\n",
        "                self.memory_deep.pop(0)\n",
        "    def get_hist(self):\n",
        "        if len(self.memory) == 0: return self.hist\n",
        "        return np.mean(self.memory, axis=0)\n",
        "    def get_hog(self):\n",
        "        if len(self.memory_hog) == 0: return self.hog_desc\n",
        "        return np.mean(self.memory_hog, axis=0)\n",
        "    def get_deep(self):\n",
        "        if len(self.memory_deep) == 0: return self.deep_feat\n",
        "        return np.mean(self.memory_deep, axis=0)\n",
        "\n",
        "#Helper Functions\n",
        "def get_distributed_kps(gray, grid, max_c, min_d):\n",
        "    h,w=gray.shape; gh,gw=grid; pts=[]\n",
        "    for i in range(gh):\n",
        "        for j in range(gw):\n",
        "            roi=gray[i*h//gh:(i+1)*h//gh, j*w//gw:(j+1)*w//gw]\n",
        "            c=cv2.goodFeaturesToTrack(roi,\n",
        "                maxCorners=max_c//(gh*gw),\n",
        "                qualityLevel=0.05, minDistance=min_d)\n",
        "            if c is not None:\n",
        "                for [[x,y]] in c:\n",
        "                    pts.append([x+j*w//gw, y+i*h//gh])\n",
        "    return np.array(pts,np.float32).reshape(-1,1,2) if pts else None\n",
        "\n",
        "def filter_pts(nw,old,max_m=25,thr=2.08,sk=100):\n",
        "    if len(nw)==0 or len(old)==0: return np.array([],bool)\n",
        "    mv=np.linalg.norm(nw-old,axis=1)<max_m\n",
        "    d =np.linalg.norm(nw-nw.mean(0),axis=1)\n",
        "    cl=d<=thr*np.mean(np.delete(d,0))\n",
        "    sp=np.linalg.norm(nw-old,axis=1)\n",
        "    sd=(sp>=sp.mean()-sk*sp.std())&(sp<=sp.mean()+sk*sp.std())\n",
        "    return mv & cl & sd\n",
        "\n",
        "#Main Script\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "ret, frame_bgr = cap.read()\n",
        "if not ret:\n",
        "    raise RuntimeError(\"Cannot read video\")\n",
        "\n",
        "frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "model = models.detection.fasterrcnn_resnet50_fpn(\n",
        "    weights=models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        ").to(DEVICE).eval()\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "img_t = transform(frame_rgb).to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = model([img_t])[0]\n",
        "\n",
        "boxes  = preds[\"boxes\"].cpu().numpy().astype(int)\n",
        "labels = preds[\"labels\"].cpu().numpy()\n",
        "scores = preds[\"scores\"].cpu().numpy()\n",
        "\n",
        "keep = scores >= CONF_TH\n",
        "boxes, labels, scores = boxes[keep], labels[keep], scores[keep]\n",
        "\n",
        "h0, w0 = frame_bgr.shape[:2]\n",
        "out = cv2.VideoWriter(\n",
        "    OUTPUT_PATH,\n",
        "    cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "    cap.get(cv2.CAP_PROP_FPS),\n",
        "    (w0, h0)\n",
        ")\n",
        "# Draw detection boxes on first frame\n",
        "for (x1,y1,x2,y2), cls_id, sc in zip(boxes, labels, scores):\n",
        "    name = CLASSES.get(int(cls_id), str(int(cls_id)))\n",
        "    label = f\"{name} {sc:.2f}\"\n",
        "    cv2.rectangle(frame_bgr, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "    (tw,th),_ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
        "    cv2.rectangle(frame_bgr, (x1,y1-th-8),(x1+tw,y1), (0,255,0), -1)\n",
        "    cv2.putText(frame_bgr, label, (x1,y1-4), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 2)\n",
        "out.write(frame_bgr)\n",
        "\n",
        "gray0 = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
        "trackers = []\n",
        "lost_tracks = []  # memory for lost tracks\n",
        "\n",
        "for i, (b, cid) in enumerate(zip(boxes, labels)):\n",
        "    hist = color_hist(frame_bgr, b)\n",
        "    hog = compute_hog(frame_bgr, b)\n",
        "    deep = extract_deep_feature(frame_bgr, b)\n",
        "    kf = KalmanBox(tuple(b), hist, hog, deep)\n",
        "    x1,y1,x2,y2 = b\n",
        "    kp = get_distributed_kps(\n",
        "        gray0[y1:y2, x1:x2],\n",
        "        GRID,\n",
        "        MAX_CORNERS[min(i,len(MAX_CORNERS)-1)],\n",
        "        MIN_DIST   [min(i,len(MIN_DIST)-1)]\n",
        "    )\n",
        "    if kp is not None and len(kp) >= MIN_LK_POINTS:\n",
        "        kp[:,:,0] += x1; kp[:,:,1] += y1\n",
        "        history = deque([kp.reshape(-1,2)], maxlen=REINIT_KP)\n",
        "    else:\n",
        "        kp, history = None, deque([], maxlen=REINIT_KP)\n",
        "    trackers.append({\n",
        "        'kf': kf,\n",
        "        'p0': kp,\n",
        "        'history': history,\n",
        "        'cls': int(cid),\n",
        "        'lost': 0,\n",
        "        'age': 0,\n",
        "        'missed': 0\n",
        "    })\n",
        "\n",
        "old_gray = gray0.copy()\n",
        "frame_count = 1\n",
        "start_time = time.time()\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret: break\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    frame_count += 1\n",
        "\n",
        "    active_ids = set()\n",
        "    for obj in trackers[:]:\n",
        "        kf = obj['kf']\n",
        "        x1,y1,x2,y2 = kf.last\n",
        "        obj['age'] += 1\n",
        "\n",
        "        # LK Optical Flow\n",
        "        if obj['p0'] is not None:\n",
        "            p1, st, _ = cv2.calcOpticalFlowPyrLK(\n",
        "                old_gray, gray, obj['p0'], None,\n",
        "                winSize=(15,15), maxLevel=2,\n",
        "                criteria=(cv2.TERM_CRITERIA_EPS|\n",
        "                          cv2.TERM_CRITERIA_COUNT,10,0.03)\n",
        "            )\n",
        "        else:\n",
        "            p1, st = None, 0\n",
        "\n",
        "        if p1 is not None and st.sum() >= MIN_LK_POINTS:\n",
        "            newp = p1[st==1].reshape(-1,2)\n",
        "            oldp = obj['p0'][st==1].reshape(-1,2)\n",
        "            pts  = newp[filter_pts(newp,oldp)]\n",
        "            if len(pts) >= MIN_LK_POINTS:\n",
        "                obj['history'].append(pts)\n",
        "                allpts = np.vstack(obj['history'])\n",
        "                hull   = cv2.convexHull(allpts.astype(np.float32))\n",
        "                bx,by,bw,bh = cv2.boundingRect(hull)\n",
        "                hist = color_hist(frame, (bx,by,bx+bw,by+bh))\n",
        "                hog  = compute_hog(frame, (bx,by,bx+bw,by+bh))\n",
        "                deep = extract_deep_feature(frame, (bx,by,bx+bw,by+bh))\n",
        "                kf.update((bx,by,bx+bw,by+bh), hist, hog, deep)\n",
        "                x1, y1, x2, y2 = bx, by, bx+bw, by+bh\n",
        "                obj['p0'] = pts.reshape(-1,1,2)\n",
        "                obj['lost'] = 0\n",
        "                obj['missed'] = 0\n",
        "                active_ids.add(kf.id)\n",
        "            else:\n",
        "                x1,y1,x2,y2 = kf.predict()\n",
        "                obj['p0'], obj['lost'] = None, obj['lost']+1\n",
        "                obj['missed'] += 1\n",
        "        else:\n",
        "            x1,y1,x2,y2 = kf.predict()\n",
        "            obj['p0'], obj['lost'] = None, obj['lost']+1\n",
        "            obj['missed'] += 1\n",
        "\n",
        "        # Wider search window for keypoint reinit\n",
        "        if obj['lost'] == REINIT_KP:\n",
        "            pad = 20  # search outside previous box\n",
        "            xx1,yy1 = max(0,x1-pad), max(0,y1-pad)\n",
        "            xx2,yy2 = min(w0,x2+pad), min(h0,y2+pad)\n",
        "            newkp = get_distributed_kps(\n",
        "                gray[yy1:yy2,xx1:xx2],\n",
        "                GRID, 2000, 5\n",
        "            )\n",
        "            if newkp is not None and len(newkp) >= MIN_LK_POINTS:\n",
        "                newkp[:,:,0]+=xx1; newkp[:,:,1]+=yy1\n",
        "                obj['history'].clear()\n",
        "                obj['history'].append(newkp.reshape(-1,2))\n",
        "                obj['p0'], obj['lost'] = newkp, 0\n",
        "\n",
        "        if obj['lost'] >= DROP_LIMIT:\n",
        "            lost_tracks.append({\n",
        "                'hist': kf.get_hist(),\n",
        "                'hog': kf.get_hog(),\n",
        "                'deep': kf.get_deep(),\n",
        "                'cls': obj['cls'],\n",
        "                'kf': kf,\n",
        "                'last_box': (x1,y1,x2,y2),\n",
        "                'last_seen': frame_count\n",
        "            })\n",
        "            trackers.remove(obj)\n",
        "            continue\n",
        "\n",
        "        color = (0,255,0)\n",
        "        if obj['p0'] is not None and len(obj['p0']) >= MIN_LK_POINTS:\n",
        "            pts = obj['p0'].reshape(-1,2)\n",
        "            hull = cv2.convexHull(pts.astype(np.float32))\n",
        "            bx, by, bw, bh = cv2.boundingRect(hull)\n",
        "            cv2.rectangle(frame, (bx,by), (bx+bw,by+bh), color, 2)\n",
        "            label = CLASSES.get(obj['cls'], str(obj['cls']))\n",
        "            cv2.putText(frame, f\"{label} #{obj['kf'].id} age:{obj['age']}\", (bx, by-6),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "            for pt in obj['p0']:\n",
        "                cv2.circle(frame, tuple(pt[0].astype(int)), 2, (0,0,255), -1)\n",
        "        else:\n",
        "            x1, y1, x2, y2 = kf.last\n",
        "            cv2.rectangle(frame, (x1,y1), (x2,y2), (255,0,0), 2)\n",
        "            label = CLASSES.get(obj['cls'], str(obj['cls']))\n",
        "            cv2.putText(frame, f\"{label} #{obj['kf'].id} age:{obj['age']}\", (x1, y1-6),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,0,0), 2)\n",
        "\n",
        "    #Improved matching for lost track recovery\n",
        "    if len(lost_tracks) > 0:\n",
        "        for lt in lost_tracks[:]:\n",
        "            if frame_count - lt['last_seen'] > REID_MAX_AGE:\n",
        "                lost_tracks.remove(lt)\n",
        "                continue\n",
        "            hist_lost = lt['hist']\n",
        "            hog_lost = lt['hog']\n",
        "            deep_lost = lt['deep']\n",
        "            best_obj = None\n",
        "            best_score = 0\n",
        "            for obj in trackers:\n",
        "                if obj['kf'].id in active_ids:\n",
        "                    continue\n",
        "                hist_obj = obj['kf'].get_hist()\n",
        "                hog_obj = obj['kf'].get_hog()\n",
        "                deep_obj = obj['kf'].get_deep()\n",
        "                sim_hist = 1.0 - hist_sim(hist_lost, hist_obj)\n",
        "                sim_hog = hog_sim(hog_lost, hog_obj)\n",
        "                sim_deep = deep_sim(deep_lost, deep_obj)\n",
        "                # Use weighted vote of all features\n",
        "                total_sim = 0.4*sim_hist + 0.3*sim_hog + 0.3*sim_deep\n",
        "                if sim_hist > (1-REID_SIM_THRESH) and sim_hog > HOG_SIM_THRESH and sim_deep > DEEP_SIM_THRESH and lt['cls'] == obj['cls']:\n",
        "                    if total_sim > best_score:\n",
        "                        best_obj = obj\n",
        "                        best_score = total_sim\n",
        "            if best_obj is not None:\n",
        "                best_obj['lost'] = 0\n",
        "                best_obj['missed'] = 0\n",
        "                best_obj['kf'].hist = hist_lost\n",
        "                best_obj['kf'].hog_desc = hog_lost\n",
        "                best_obj['kf'].deep_feat = deep_lost\n",
        "                best_obj['kf'].memory.append(hist_lost)\n",
        "                best_obj['kf'].memory_hog.append(hog_lost)\n",
        "                best_obj['kf'].memory_deep.append(deep_lost)\n",
        "                lost_tracks.remove(lt)\n",
        "                print(f\"Re-identified track #{best_obj['kf'].id} at frame {frame_count}\")\n",
        "\n",
        "    out.write(frame)\n",
        "    old_gray = gray.copy()\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "fps = frame_count / total_time\n",
        "print(f\"âœ… Done â€“ saved to {OUTPUT_PATH}\")\n",
        "print(f\"Average FPS: {fps:.2f}\")"
      ],
      "metadata": {
        "id": "c9iZ8qP8OqGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CSRT_version2\n",
        "import cv2\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "video_path = \"car2.mp4\"\n",
        "output_video_path = \"car2_csrt.mp4\"\n",
        "target_object_name = \"car\"\n",
        "x_frames = 50\n",
        "\n",
        "try:\n",
        "    model = YOLO('yolov8n.pt')\n",
        "    class_names = list(model.names.values())\n",
        "except Exception as e:\n",
        "    print(f\"Error loading YOLO model: {e}\")\n",
        "    class_names = [\"person\", \"car\", \"dog\", \"cat\"]\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "if not cap.isOpened():\n",
        "    raise FileNotFoundError(f\"Error: Could not open video at {video_path}\")\n",
        "\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "tracker = None\n",
        "frame_count = 0\n",
        "detection_frame_count = 0\n",
        "total_processing_time = 0\n",
        "tracking_frame_count = 0\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_count += 1\n",
        "    start_time = time.time()\n",
        "\n",
        "    if tracker is None:\n",
        "        if frame_count <= x_frames:\n",
        "            results = model(frame, stream=True, verbose=False)\n",
        "            for r in results:\n",
        "                boxes = r.boxes\n",
        "                for box in boxes:\n",
        "                    cls = int(box.cls[0])\n",
        "                    label = model.names[cls]\n",
        "                    if label == target_object_name:\n",
        "                        x1, y1, x2, y2 = box.xyxy[0]\n",
        "                        bbox = (int(x1), int(y1), int(x2 - x1), int(y2 - y1))\n",
        "                        tracker = cv2.TrackerCSRT_create()\n",
        "                        tracker.init(frame, bbox)\n",
        "                        print(f\"Target '{target_object_name}' found at {bbox}. Starting tracker.\")\n",
        "                        total_processing_time = 0\n",
        "                        tracking_frame_count = 0\n",
        "                        break\n",
        "                if tracker:\n",
        "                    break\n",
        "        else:\n",
        "            print(f\"\\nCould not find '{target_object_name}' in the first {x_frames} frames. Exiting.\")\n",
        "            out.write(frame)\n",
        "            break\n",
        "    else:\n",
        "        success, bbox = tracker.update(frame)\n",
        "        tracking_frame_count += 1\n",
        "        if success:\n",
        "            x, y, w, h = [int(v) for v in bbox]\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "            # Label above box\n",
        "            cv2.putText(frame, target_object_name, (x, y - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "        else:\n",
        "            cv2.putText(frame, \"Tracking Failure\", (100, 80),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
        "\n",
        "        end_time = time.time()\n",
        "        total_processing_time += (end_time - start_time)\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "if tracking_frame_count > 0 and total_processing_time > 0:\n",
        "    avg_fps = tracking_frame_count / total_processing_time\n",
        "    print(f\"\\nâœ… Done! Output saved to '{output_video_path}'\")\n",
        "    print(f\"ðŸ“ˆ Average Tracking FPS: {avg_fps:.2f}\")\n",
        "else:\n",
        "    print(\"\\nNo tracking data to compute FPS.\")"
      ],
      "metadata": {
        "id": "t5eizb43_9T_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}